בהתבסס על השיחה ועל הדרישות שהוצגו, אני מציג להלן את הפרויקט המוגמר המלא של לוח הבקרה לניטור רמות הים (Sea Level Dashboard), לאחר ריפקטורינג משמעותי לארכיטקטורת מיקרו-שירותים מבוססת AWS, עם חלוקה ברורה ל-Frontend ו-Backend. הפרויקט כולל את כל הקבצים, ההגדרות והשלבים הנדרשים, ללא השמטת פרטים, כפי שביקשת.

---

## **מבנה הפרויקט המוגמר**

הפרויקט מחולק לשני חלקים עיקריים:  
- **`frontend/`**: ממשק המשתמש, כתוב ב-React.  
- **`backend/`**: שירותי ה-Backend, מפוצלים לשכבה משותפת ולפונקציות Lambda נפרדות.

```
sea-level-project/
├── frontend/
│   ├── public/
│   │   ├── index.html
│   │   └── assets/
│   │       ├── Mapi_Logo2.png
│   │       └── style.css
│   ├── src/
│   │   ├── components/
│   │   │   ├── Filters.js
│   │   │   ├── StatsCards.js
│   │   │   ├── GraphView.js
│   │   │   ├── TableView.js
│   │   │   └── MapView.js
│   │   ├── App.js
│   │   ├── App.css
│   │   └── index.js
│   └── package.json
│
└── backend/
    ├── shared/
    │   ├── database.py
    │   ├── data_processing.py
    │   └── utils.py
    │
    └── lambdas/
        ├── get_stations/
        │   ├── main.py
        │   └── requirements.txt
        ├── get_yesterday_data/
        │   ├── main.py
        │   └── requirements.txt
        ├── get_live_data/
        │   ├── main.py
        │   └── requirements.txt
        ├── get_data/
        │   ├── main.py
        │   └── requirements.txt
        ├── get_predictions/
        │   ├── main.py
        │   └── requirements.txt
        └── get_station_map/
            ├── main.py
            └── requirements.txt
```

---

## **Backend - הקוד המלא**

### **שכבה משותפת (Shared Layer)**

השכבה המשותפת מרכזת את כל הלוגיקה המשותפת לפונקציות ה-Lambda, כגון חיבור לבסיס הנתונים, עיבוד נתונים ופונקציות עזר.

#### **`backend/shared/database.py`**

```python
# backend/shared/database.py
import os
import logging
from sqlalchemy import create_engine, MetaData, Table, Column
from sqlalchemy.types import TypeDecorator, String
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Database URI from environment
DB_URI = os.getenv('DB_URI')
if not DB_URI:
    raise RuntimeError("DB_URI not set. Check environment variables.")

# Create SQLAlchemy engine
engine = create_engine(DB_URI, pool_pre_ping=True, pool_size=5, max_overflow=10, pool_recycle=1800)
metadata = MetaData()

# Custom TypeDecorator for PostgreSQL POINT type
class PointType(TypeDecorator):
    impl = String
    cache_ok = True

    def process_bind_param(self, value, dialect):
        return f"{value[0]},{value[1]}" if value else None

    def process_result_value(self, value, dialect):
        return tuple(map(float, value.split(','))) if value else None

# Define database tables
try:
    M = Table('Monitors_info2', metadata,
              Column('Tab_TabularTag', String),
              autoload_with=engine,
              extend_existing=True)
    
    L = Table('Locations', metadata,
              Column('locations', PointType()),
              Column('Station', String),
              autoload_with=engine,
              extend_existing=True)
    
    S = Table('SeaTides', metadata,
              autoload_with=engine,
              extend_existing=True)
    
    logging.info("Database tables loaded successfully.")
except Exception as e:
    logging.error(f"Database initialization failed: {e}")
    M, L, S = None, None, None
```

#### **`backend/shared/data_processing.py`**

```python
# backend/shared/data_processing.py
import pandas as pd
import numpy as np
from sqlalchemy import select, and_, text, func
from datetime import datetime, timedelta
from statsmodels.tsa.arima.model import ARIMA
from prophet import Prophet
from sklearn.ensemble import IsolationForest
from .database import engine, M, L, S

# Data Loading
def load_data_from_db(start_date=None, end_date=None, station=None, data_source='default'):
    if engine is None or M is None or L is None or S is None:
        logging.error("Database engine or tables not loaded. Cannot query data.")
        return pd.DataFrame()

    try:
        sql_query_obj = build_query(start_date, end_date, station, data_source)
        if sql_query_obj is None:
            logging.error("build_query returned None. Cannot execute query.")
            return pd.DataFrame()

        df_chunks = []
        with engine.connect() as connection:
            result = connection.execute(sql_query_obj)
            while True:
                chunk = result.fetchmany(10000)
                if not chunk:
                    break
                df_chunk = pd.DataFrame(chunk, columns=result.keys())
                df_chunks.append(df_chunk)

        if not df_chunks:
            return pd.DataFrame()
        return pd.concat(df_chunks, ignore_index=True)

    except Exception as e:
        logging.error(f"Data load error: {e}")
        return pd.DataFrame()

def build_query(start_date, end_date, station, data_source):
    params = {}
    if start_date:
        params['start_date'] = start_date
    if end_date:
        params['end_date'] = end_date
    if station and station != 'All Stations':
        params['station'] = station

    stmt = None
    table_for_date_filter = None
    date_col_name = None

    if data_source == 'tides':
        cols_to_select = tides_columns()
        table_for_date_filter = S
        date_col_name = 'Date'
        stmt = select(*cols_to_select).select_from(S)
    else:
        cols_to_select = default_columns()
        join_condition = M.c.Tab_TabularTag == L.c.Tab_TabularTag
        stmt = select(*cols_to_select).select_from(M.join(L, join_condition))
        table_for_date_filter = M
        date_col_name = 'Tab_DateTime'

    if start_date:
        stmt = stmt.where(table_for_date_filter.c[date_col_name] >= params['start_date'])
    if end_date:
        stmt = stmt.where(table_for_date_filter.c[date_col_name] <= params['end_date'])

    if data_source == 'default' and 'station' in params:
        stmt = stmt.where(L.c.Station == params['station'])
    elif data_source == 'tides' and 'station' in params:
        stmt = stmt.where(S.c.Station == params['station'])

    if date_col_name and table_for_date_filter is not None:
        stmt = stmt.order_by(table_for_date_filter.c[date_col_name])

    return stmt

def default_columns():
    return [M.c.Tab_DateTime, L.c.Station, M.c.Tab_Value_mDepthC1, M.c.Tab_Value_monT2m]

def tides_columns():
    return [S.c.Date, S.c.Station, S.c.HighTide, S.c.High_', S.c.LowTideTime, S.c.LowTideTemp, S.c.MeasurementCount]

# Prediction Functions
def get_prediction_data(station):
    end_date = datetime.now()
    start_date = end_date - timedelta(days=365)
    return load_data_from_db(
        start_date=start_date.strftime('%Y-%m-%d'),
        end_date=end_date.strftime('%Y-%m-%d'),
        station=station,
        data_source='default'
    )

def arima_predict(station):
    try:
        df = get_prediction_data(station)
        if df.empty or 'Tab_Value_mDepthC1' not in df.columns:
            logging.warning(f"No data available for ARIMA prediction for station {station}.")
            return None

        df['Tab_DateTime'] = pd.to_datetime(df['Tab_DateTime'])
        df = df.sort_values('Tab_DateTime')
        df = df.set_index('Tab_DateTime')

        series_to_predict = df['Tab_Value_mDepthC1'].resample('h').mean().dropna()
        if len(series_to_predict) < 20:
            logging.warning(f"Not enough data points ({len(series_to_predict)}) for ARIMA prediction for station {station}.")
            return None

        model = ARIMA(series_to_predict, order=(5, 1, 0))
        model_fit = model.fit()
        forecast = model_fit.forecast(steps=240)
        return forecast.tolist()
    except Exception as e:
        logging.error(f"ARIMA prediction failed for station {station}: {str(e)}")
        return None

def prophet_predict(station):
    try:
        df = get_prediction_data(station)
        if df.empty or 'Tab_Value_mDepthC1' not in df.columns:
            logging.warning(f"No data available for Prophet prediction for station {station}.")
            return pd.DataFrame()

        df['Tab_DateTime'] = pd.to_datetime(df['Tab_DateTime'])
        df = df.sort_values('Tab_DateTime')
        df = df.set_index('Tab_DateTime')

        prophet_df = df['Tab_Value_mDepthC1'].resample('h').mean().reset_index()
        prophet_df = prophet_df.rename(columns={'Tab_DateTime': 'ds', 'Tab_Value_mDepthC1': 'y'})[['ds', 'y']].dropna()

        if len(prophet_df) < 50:
            logging.warning(f"Not enough data points ({len(prophet_df)}) for Prophet prediction for station {station}.")
            return pd.DataFrame()

        model = Prophet(yearly_seasonality=True, daily_seasonality=True, growth='linear')
        model.fit(prophet_df)
        future = model.make_future_dataframe(periods=240, freq='h')

        if future.empty:
            logging.warning(f"Future dataframe is empty for station {station}.")
            return pd.DataFrame()

        forecast = model.predict(future)
        if forecast is None or forecast.empty or 'yhat' not in forecast.columns:
            logging.warning(f"Forecast result is invalid for station {station}.")
            return pd.DataFrame()

        return forecast[['ds', 'yhat']]
    except Exception as e:
        logging.error(f"Prophet prediction failed for station {station}: {str(e)}")
        return pd.DataFrame()

# Anomaly Detection
def detect_anomalies(df):
    iso_forest = IsolationForest(contamination=0.01, random_state=42)
    if df.empty or 'Tab_Value_mDepthC1' not in df.columns:
        return df

    X = df[['Tab_Value_mDepthC1']].values
    pred = iso_forest.fit_predict(X)
    df['anomaly'] = np.where(pred == -1, -1, 0)
    return df

# Stats Calculation
def calculate_stats(df):
    stats = {
        'current_level': None,
        '24h_change': None,
        'avg_temp': None,
        'anomalies': None
    }

    if not df.empty:
        try:
            if 'Tab_Value_mDepthC1' in df.columns:
                stats['current_level'] = df['Tab_Value_mDepthC1'].iloc[-1]

            if 'Tab_Value_mDepthC1' in df.columns and len(df) > 1:
                now_val = df['Tab_Value_mDepthC1'].iloc[-1]
                yesterday_val = df['Tab_Value_mDepthC1'].iloc[0]
                stats['24h_change'] = now_val - yesterday_val

            if 'Tab_Value_monT2m' in df.columns:
                stats['avg_temp'] = df['Tab_Value_monT2m'].mean()

            if 'anomaly' in df.columns:
                stats['anomalies'] = df[df['anomaly'] == -1].shape[0]
        except Exception as e:
            logging.error(f"Error calculating stats: {e}")

    return stats
```

#### **`backend/shared/utils.py`**

```python
# backend/shared/utils.py
import re
from datetime import datetime

def generate_export_filename(station, start_date, end_date, extension="png"):
    station = station or "AllStations"
    sanitized_station = re.sub(r'[^\w\-]', '', station)

    def format_date(date_str):
        if not date_str:
            return "NODATE"
        try:
            if ' ' in date_str:
                date_part = date_str.split(' ')[0]
                return datetime.strptime(date_part, '%Y-%m-%d').strftime('%Y-%m-%d')
            return datetime.strptime(date_str, '%Y-%m-%d').strftime('%Y-%m-%d')
        except:
            return re.sub(r'[^\w\-]', '_', date_str)[:20]

    sanitized_start = format_date(start_date)
    sanitized_end = format_date(end_date)

    return f"sea_level_{sanitized_station}_{sanitized_start}_to_{sanitized_end}.{extension}"
```

---

### **פונקציות Lambda**

כל endpoint מה-API המקורי הומר לפונקציית Lambda נפרדת, תוך שימוש בשכבה המשותפת.

#### **`backend/lambdas/get_stations/main.py`**

```python
# backend/lambdas/get_stations/main.py
import json
import logging
from shared.database import engine, L
from sqlalchemy import select

logging.basicConfig(level=logging.INFO)

def get_all_stations_from_db():
    try:
        with engine.connect() as connection:
            query = select(L.c.Station).distinct().order_by(L.c.Station)
            result = connection.execute(query)
            stations = [row[0] for row in result if row[0] is not None]
            return ['All Stations'] + stations
    except Exception as e:
        logging.error(f"Error fetching stations: {e}")
        return []

def handler(event, context):
    try:
        stations = get_all_stations_from_db()
        return {
            "statusCode": 200,
            "headers": {
                "Content-Type": "application/json",
                "Access-Control-Allow-Origin": "*"
            },
            "body": json.dumps({"stations": stations})
        }
    except Exception as e:
        logging.error(f"Error in get_stations lambda: {e}")
        return {
            "statusCode": 500,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps({"error": "Internal server error"})
        }
```

**`backend/lambdas/get_stations/requirements.txt`**

```
sqlalchemy==2.0.30
psycopg2-binary==2.9.9
python-dotenv==1.0.1
```

#### **`backend/lambdas/get_yesterday_data/main.py`**

```python
# backend/lambdas/get_yesterday_data/main.py
import json
import logging
from datetime import datetime, timedelta
from shared.data_processing import load_data_from_db

logging.basicConfig(level=logging.INFO)

def handler(event, context):
    try:
        path_params = event.get('pathParameters') or {}
        station = path_params.get('station')
        if not station:
            return {
                "statusCode": 400,
                "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
                "body": json.dumps({"error": "Station parameter is required"})
            }

        today = datetime.utcnow().date()
        yesterday = today - timedelta(days=1)
        start_date = datetime.combine(yesterday, datetime.min.time()).strftime('%Y-%m-%d %H:%M:%S')
        end_date = datetime.combine(yesterday, datetime.max.time()).strftime('%Y-%m-%d %H:%M:%S')

        df = load_data_from_db(start_date=start_date, end_date=end_date, station=station, data_source='default')
        response_body = df.to_json(orient='records', date_format='iso')

        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
            "body": response_body
        }
    except Exception as e:
        logging.error(f"Error in get_yesterday_data lambda: {e}")
        return {
            "statusCode": 500,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps({"error": "Internal server error"})
        }
```

**`backend/lambdas/get_yesterday_data/requirements.txt`**

```
pandas==2.2.2
sqlalchemy==2.0.30
psycopg2-binary==2.9.9
python-dotenv==1.0.1
```

#### **`backend/lambdas/get_live_data/main.py`**

```python
# backend/lambdas/get_live_data/main.py
import json
import logging
from sqlalchemy import select, func, and_
from shared.database import engine, M, L

logging.basicConfig(level=logging.INFO)

def handler(event, context):
    try:
        path_params = event.get('pathParameters') or {}
        station = path_params.get('station')

        latest_time_subq = (
            select([
                L.c.Station,
                func.max(M.c.Tab_DateTime).label('max_time')
            ])
            .select_from(M.join(L, M.c.Tab_TabularTag == L.c.Tab_TabularTag))
            .group_by(L.c.Station)
            .subquery()
        )

        stmt = (
            select([
                L.c.Station,
                M.c.Tab_Value_mDepthC1,
                M.c.Tab_DateTime
            ])
            .select_from(
                M.join(L, M.c.Tab_TabularTag == L.c.Tab_TabularTag)
                .join(latest_time_subq, and_(
                    L.c.Station == latest_time_subq.c.Station,
                    M.c.Tab_DateTime == latest_time_subq.c.max_time
                ))
            )
        )

        if station:
            stmt = stmt.where(L.c.Station == station)

        with engine.connect() as conn:
            result = conn.execute(stmt)
            columns = result.keys()
            rows = result.fetchall()

        data = [dict(zip(columns, row)) for row in rows]
        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
            "body": json.dumps({"station": station or 'all', "data": data})
        }
    except Exception as e:
        logging.error(f"Error in get_live_data lambda: {e}")
        return {
            "statusCode": 500,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps({"error": "Internal server error"})
        }
```

**`backend/lambdas/get_live_data/requirements.txt`**

```
sqlalchemy==2.0.30
psycopg2-binary==2.9.9
python-dotenv==1.0.1
```

#### **`backend/lambdas/get_data/main.py`**

```python
# backend/lambdas/get_data/main.py
import json
import logging
from shared.data_processing import load_data_from_db, detect_anomalies

logging.basicConfig(level=logging.INFO)

def handler(event, context):
    try:
        params = event.get('queryStringParameters') or {}
        station = params.get('station', 'All Stations')
        start_date = params.get('start_date')
        end_date = params.get('end_date')
        data_source = params.get('data_source', 'default')
        show_anomalies = params.get('show_anomalies', 'false').lower() == 'true'

        if station == 'All Stations':
            station = None

        if data_source == 'default' and not station:
            return {
                "statusCode": 400,
                "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
                "body": json.dumps({"error": "Station parameter is required for default data source"})
            }

        df = load_data_from_db(start_date, end_date, station, data_source)

        if show_anomalies and data_source == 'default' and not df.empty:
            df = detect_anomalies(df.copy())
        else:
            if 'anomaly' not in df.columns:
                df['anomaly'] = 0

        if df.empty:
            return {
                "statusCode": 404,
                "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
                "body": json.dumps({
                    "message": "No data found",
                    "details": {
                        "station": station or "All Stations",
                        "start_date": start_date,
                        "end_date": end_date,
                        "data_source": data_source
                    }
                })
            }

        response_body = df.to_json(orient='records', date_format='iso')
        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
            "body": response_body
        }
    except Exception as e:
        logging.error(f"Error in get_data lambda: {e}")
        return {
            "statusCode": 500,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps({"error": "Internal server error"})
        }
```

**`backend/lambdas/get_data/requirements.txt`**

```
pandas==2.2.2
sqlalchemy==2.0.30
psycopg2-binary==2.9.9
python-dotenv==1.0.1
scikit-learn==1.4.2
```

#### **`backend/lambdas/get_predictions/main.py`**

```python
# backend/lambdas/get_predictions/main.py
import json
import logging
from shared.data_processing import arima_predict, prophet_predict

logging.basicConfig(level=logging.INFO)

def handler(event, context):
    try:
        params = event.get('queryStringParameters') or {}
        station = params.get('station')
        model = params.get('model', 'all')

        if not station:
            return {
                "statusCode": 400,
                "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
                "body": json.dumps({"error": "Station parameter is required"})
            }

        results = {}
        if model in ['arima', 'all']:
            arima_pred = arima_predict(station)
            results['arima'] = arima_pred if arima_pred else None

        if model in ['prophet', 'all']:
            prophet_pred = prophet_predict(station)
            if prophet_pred is not None and not prophet_pred.empty:
                results['prophet'] = prophet_pred.to_dict(orient='records')
            else:
                results['prophet'] = None

        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
            "body": json.dumps(results)
        }
    except Exception as e:
        logging.error(f"Error in get_predictions lambda: {e}")
        return {
            "statusCode": 500,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps({"error": "Internal server error"})
        }
```

**`backend/lambdas/get_predictions/requirements.txt`**

```
pandas==2.2.2
sqlalchemy==2.0.30
psycopg2-binary==2.9.9
python-dotenv==1.0.1
statsmodels==0.14.2
prophet==1.1.5
```

#### **`backend/lambdas/get_station_map/main.py`**

```python
# backend/lambdas/get_station_map/main.py
import json
import logging
import pandas as pd
from sqlalchemy import text
from shared.database import engine
from pyproj import Transformer

logging.basicConfig(level=logging.INFO)

def fetch_station_locations():
    try:
        query = text("""
            SELECT DISTINCT "Station", 
                   (locations::point)[0] AS latitude, 
                   (locations::point)[1] AS longitude
            FROM "Locations" 
            WHERE locations IS NOT NULL
        """)
        with engine.connect() as connection:
            result = connection.execute(query)
            df = pd.DataFrame(result.fetchall(), columns=result.keys())

        if df.empty:
            logging.warning("No station location data fetched for map")
            return pd.DataFrame(columns=["Station", "longitude", "latitude"])

        df['latitude'] = df['latitude'].astype(str).str.replace("(", "", regex=False).str.replace(")", "", regex=False).astype(float)
        df['longitude'] = df['longitude'].astype(str).str.replace("(", "", regex=False).str.replace(")", "", regex=False).astype(float)
        return df
    except Exception as e:
        logging.error(f"Error fetching locations: {e}")
        return pd.DataFrame(columns=["Station", "longitude", "latitude"])

def handler(event, context):
    try:
        locations_df = fetch_station_locations()
        if locations_df.empty:
            return {
                "statusCode": 404,
                "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
                "body": json.dumps({"message": "No station location data available"})
            }

        transformer = Transformer.from_crs("epsg:4326", "epsg:2039", always_xy=True)
        locations_df[['x', 'y']] = locations_df.apply(
            lambda row: pd.Series(transformer.transform(row['longitude'], row['latitude'])),
            axis=1
        )

        locations_df = locations_df.dropna(subset=['x', 'y'])
        locations_df = locations_df.sort_values(by='Station')

        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
            "body": json.dumps(locations_df[['Station', 'x', 'y']].to_dict(orient='records'))
        }
    except Exception as e:
        logging.error(f"Error in get_station_map lambda: {e}")
        return {
            "statusCode": 500,
            "headers": {"Content-Type": "application/json"},
            "body": json.dumps({"error": "Internal server error"})
        }
```

**`backend/lambdas/get_station_map/requirements.txt`**

```
pandas==2.2.2
sqlalchemy==2.0.30
psycopg2-binary==2.9.9
python-dotenv==1.0.1
pyproj==3.6.1
```

---

## **Frontend - הקוד המלא**

ה-Frontend נכתב מחדש ב-React, תוך שימוש ב-`axios` לקריאות API, `plotly.js` לגרפים ו-`react-leaflet` למפות.

### **`frontend/package.json`**

```json
{
  "name": "sea-level-frontend",
  "version": "1.0.0",
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "axios": "^1.6.0",
    "plotly.js": "^2.27.0",
    "react-plotly.js": "^2.6.0",
    "react-leaflet": "^4.2.1",
    "leaflet": "^1.9.4",
    "react-bootstrap": "^2.9.0",
    "bootstrap": "^5.3.2",
    "react-date-picker": "^10.5.0",
    "moment": "^2.29.4",
    "amazon-cognito-identity-js": "^6.3.6"
  },
  "scripts": {
    "start": "react-scripts start",
    "build": "react-scripts build",
    "test": "react-scripts test",
    "eject": "react-scripts eject"
  },
  "devDependencies": {
    "react-scripts": "5.0.1"
  }
}
```

### **`frontend/public/index.html`**

```html
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Sea Level Dashboard</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
  </body>
</html>
```

### **`frontend/src/index.js`**

```javascript
import React from 'react';
import ReactDOM from 'react-dom';
import App from './App';
import 'bootstrap/dist/css/bootstrap.min.css';

ReactDOM.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
  document.getElementById('root')
);
```

### **`frontend/src/App.js`**

```jsx
import React, { useState, useEffect } from 'react';
import { Container, Navbar, Nav, Tabs, Tab } from 'react-bootstrap';
import Filters from './components/Filters';
import StatsCards from './components/StatsCards';
import GraphView from './components/GraphView';
import TableView from './components/TableView';
import MapView from './components/MapView';
import './App.css';
import logo from '../public/assets/Mapi_Logo2.png';

const API_BASE_URL = 'https://YOUR_API_ID.execute-api.us-east-1.amazonaws.com/prod';

function App() {
  const [activeTab, setActiveTab] = useState('graph-tab');
  const [mapType, setMapType] = useState('osm-tab');
  const [stations, setStations] = useState([]);
  const [filters, setFilters] = useState({
    startDate: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000),
    endDate: new Date(),
    station: 'All Stations',
    dataType: 'default',
    trendline: 'none',
    analysisType: 'none',
    showAnomalies: false,
    predictionModels: []
  });
  const [stats, setStats] = useState({ current_level: null, '24h_change': null, avg_temp: null, anomalies: null });
  const [currentTime, setCurrentTime] = useState(new Date());

  useEffect(() => {
    const fetchStations = async () => {
      try {
        const response = await fetch(`${API_BASE_URL}/stations`);
        const data = await response.json();
        setStations(data.stations);
      } catch (error) {
        console.error('Error fetching stations:', error);
      }
    };
    fetchStations();

    const timer = setInterval(() => setCurrentTime(new Date()), 1000);
    return () => clearInterval(timer);
  }, []);

  return (
    <div className="dash-container">
      <Navbar className="header">
        <Container>
          <Navbar.Brand>
            <img src={logo} alt="Mapi Logo" style={{ height: '60px', marginRight: '15px' }} />
            <h1>Sea Level Monitoring System</h1>
          </Navbar.Brand>
          <div id="current-time">{currentTime.toLocaleString()}</div>
        </Container>
      </Navbar>

      <Container fluid className="main-content">
        <div className="filters-column">
          <Filters
            stations={stations}
            filters={filters}
            setFilters={setFilters}
            apiBaseUrl={API_BASE_URL}
          />
        </div>

        <div className="content-area">
          <StatsCards stats={stats} />
          
          <Tabs
            id="view-tabs"
            activeKey={activeTab}
            onSelect={(k) => setActiveTab(k)}
            className="tab-header"
          >
            <Tab eventKey="graph-tab" title="Graph View" className="tab-panel">
              <div className="graph-panel">
                <GraphView filters={filters} apiBaseUrl={API_BASE_URL} setStats={setStats} />
              </div>
            </Tab>
            <Tab eventKey="table-tab" title="Table View" className="tab-panel">
              <div className="table-panel">
                <TableView filters={filters} apiBaseUrl={API_BASE_URL} />
              </div>
            </Tab>
            <Tab eventKey="map-tab" title="Map View" className="tab-panel">
              <div className="map-panel">
                <Tabs
                  id="map-type-tabs"
                  activeKey={mapType}
                  onSelect={(k) => setMapType(k)}
                  className="mb-3"
                >
                  <Tab eventKey="osm-tab" title="OSM Map">
                    <MapView filters={filters} apiBaseUrl={API_BASE_URL} />
                  </Tab>
                  <Tab eventKey="govmap-tab" title="GovMap">
                    <iframe
                      src={`${API_BASE_URL}/mapframe`}
                      style={{ width: '100%', height: '500px', border: 'none' }}
                    />
                  </Tab>
                </Tabs>
              </div>
            </Tab>
          </Tabs>
        </div>
      </Container>
    </div>
  );
}

export default App;
```

### **`frontend/src/App.css`**

```css
/* Reuse the original style.css from the monolithic app */
@import url('../public/assets/style.css');
```

### **`frontend/public/assets/style.css`**

העתק את תוכן הקובץ `style.css` מהפרויקט המקורי לכאן (ראה קובץ זה בתיעוד המקורי).

### **`frontend/src/components/Filters.js`**

```jsx
import React from 'react';
import { Form, Button } from 'react-bootstrap';
import DatePicker from 'react-date-picker';
import moment from 'moment';

const Filters = ({ stations, filters, setFilters, apiBaseUrl }) => {
  const handleExportGraph = async () => {
    const response = await fetch(`${apiBaseUrl}/data?station=${filters.station}&start_date=${moment(filters.startDate).format('YYYY-MM-DD')}&end_date=${moment(filters.endDate).format('YYYY-MM-DD')}&data_source=${filters.dataType}&show_anomalies=${filters.showAnomalies}`);
    const data = await response.json();
    const filename = `sea_level_${filters.station || 'AllStations'}_${moment(filters.startDate).format('YYYY-MM-DD')}_to_${moment(filters.endDate).format('YYYY-MM-DD')}.json`;
    const blob = new Blob([JSON.stringify(data)], { type: 'application/json' });
    const url = window.URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = filename;
    a.click();
    window.URL.revokeObjectURL(url);
  };

  const handleExportTable = async () => {
    const response = await fetch(`${apiBaseUrl}/data?station=${filters.station}&start_date=${moment(filters.startDate).format('YYYY-MM-DD')}&end_date=${moment(filters.endDate).format('YYYY-MM-DD')}&data_source=${filters.dataType}`);
    const data = await response.json();
    const csv = convertToCSV(data);
    const filename = `sea_level_${filters.station || 'AllStations'}_${moment(filters.startDate).format('YYYY-MM-DD')}_to_${moment(filters.endDate).format('YYYY-MM-DD')}.csv`;
    const blob = new Blob([csv], { type: 'text/csv' });
    const url = window.URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = filename;
    a.click();
    window.URL.revokeObjectURL(url);
  };

  const convertToCSV = (data) => {
    if (!data || data.length === 0) return '';
    const headers = Object.keys(data[0]);
    const rows = data.map(row => headers.map(header => JSON.stringify(row[header])).join(','));
    return [headers.join(','), ...rows].join('\n');
  };

  return (
    <div>
      <h3>Data Filters</h3>
      <Form>
        <Form.Group>
          <Form.Label>Date Range:</Form.Label>
          <div>
            <DatePicker
              onChange={(date) => setFilters({ ...filters, startDate: date })}
              value={filters.startDate}
              format="yyyy-MM-dd"
            />
            <DatePicker
              onChange={(date) => setFilters({ ...filters, endDate: date })}
              value={filters.endDate}
              format="yyyy-MM-dd"
            />
          </div>
        </Form.Group>

        <Form.Group>
          <Form.Label>Station Selection:</Form.Label>
          <Form.Select
            value={filters.station}
            onChange={(e) => setFilters({ ...filters, station: e.target.value })}
          >
            {stations.map((station) => (
              <option key={station} value={station}>{station}</option>
            ))}
          </Form.Select>
        </Form.Group>

        <Form.Group>
          <Form.Label>Data Type:</Form.Label>
          <Form.Select
            value={filters.dataType}
            onChange={(e) => setFilters({ ...filters, dataType: e.target.value })}
          >
            <option value="default">Default Sensor Data</option>
            <option value="tides">Tidal Data</option>
          </Form.Select>
        </Form.Group>

        <Form.Group>
          <Form.Label>Trendline Period:</Form.Label>
          <Form.Select
            value={filters.trendline}
            onChange={(e) => setFilters({ ...filters, trendline: e.target.value })}
          >
            <option value="none">No Trendline</option>
            <option value="all">All Period</option>
            <option value="last_two_decades">Last Two Decades</option>
            <option value="last_decade">Last Decade</option>
          </Form.Select>
        </Form.Group>

        <Form.Group>
          <Form.Label>Analysis Type:</Form.Label>
          <Form.Select
            value={filters.analysisType}
            onChange={(e) => setFilters({ ...filters, analysisType: e.target.value })}
          >
            <option value="none">None</option>
            <option value="rolling_avg_3h">3-Hour Rolling Avg</option>
            <option value="rolling_avg_6h">6-Hour Rolling Avg</option>
            <option value="rolling_avg_24h">24-Hour Rolling Avg</option>
            <option value="all">All Rolling Averages</option>
          </Form.Select>
        </Form.Group>

        <Form.Group>
          <Form.Check
            type="switch"
            label="Show Anomalies"
            checked={filters.showAnomalies}
            onChange={(e) => setFilters({ ...filters, showAnomalies: e.target.checked })}
          />
        </Form.Group>

        <Form.Group>
          <Form.Label>Prediction Models:</Form.Label>
          <Form.Check
            type="switch"
            label="ARIMA"
            checked={filters.predictionModels.includes('arima')}
            onChange={(e) => {
              const models = e.target.checked
                ? [...filters.predictionModels, 'arima']
                : filters.predictionModels.filter(m => m !== 'arima');
              setFilters({ ...filters, predictionModels: models });
            }}
          />
          <Form.Check
            type="switch"
            label="Prophet"
            checked={filters.predictionModels.includes('prophet')}
            onChange={(e) => {
              const models = e.target.checked
                ? [...filters.predictionModels, 'prophet']
                : filters.predictionModels.filter(m => m !== 'prophet');
              setFilters({ ...filters, predictionModels: models });
            }}
          />
        </Form.Group>

        <div className="export-buttons">
          <Button onClick={handleExportGraph} className="w-50">Export Graph</Button>
          <Button onClick={handleExportTable} className="w-50">Export Table</Button>
        </div>
      </Form>
    </div>
  );
};

export default Filters;
```

### **`frontend/src/components/StatsCards.js`**

```jsx
import React from 'react';

const StatsCards = ({ stats }) => {
  return (
    <div className="kpi-row">
      <div className="kpi-card">
        <div className="kpi-label">Current Level</div>
        <div className="kpi-value">{stats.current_level ? `${stats.current_level.toFixed(3)} m` : 'N/A'}</div>
      </div>
      <div className={`kpi-card ${stats['24h_change'] && stats['24h_change'] >= 0 ? 'green' : 'red'}`}>
        <div className="kpi-label">24h Change</div>
        <div className="kpi-value">{stats['24h_change'] ? `${stats['24h_change'].toFixed(3)} m` : 'N/A'}</div>
      </div>
      <div className="kpi-card">
        <div className="kpi-label">Avg. Temp</div>
        <div className="kpi-value">{stats.avg_temp ? `${stats.avg_temp.toFixed(1)}°C` : 'N/A'}</div>
      </div>
      <div className="kpi-card">
        <div className="kpi-label">Anomalies</div>
        <div className="kpi-value">{stats.anomalies !== null ? stats.anomalies : 'N/A'}</div>
      </div>
    </div>
  );
};

export default StatsCards;
```

### **`frontend/src/components/GraphView.js`**

```jsx
import React, { useState, useEffect } from 'react';
import Plot from 'react-plotly.js';
import moment from 'moment';

const GraphView = ({ filters, apiBaseUrl, setStats }) => {
  const [graphData, setGraphData] = useState([]);

  useEffect(() => {
    const fetchData = async () => {
      try {
        const response = await fetch(
          `${apiBaseUrl}/data?station=${filters.station}&start_date=${moment(filters.startDate).format('YYYY-MM-DD')}&end_date=${moment(filters.endDate).format('YYYY-MM-DD')}&data_source=${filters.dataType}&show_anomalies=${filters.showAnomalies}`
        );
        const data = await response.json();
        if (!data || data.length === 0) {
          setGraphData([]);
          setStats({ current_level: null, '24h_change': null, avg_temp: null, anomalies: null });
          return;
        }

        let traces = [];
        if (filters.dataType === 'default') {
          const df = data.map(item => ({
            ...item,
            Tab_DateTime: new Date(item.Tab_DateTime)
          }));

          if (filters.station === 'All Stations') {
            const stations = [...new Set(df.map(item => item.Station))];
            stations.forEach(station => {
              const stationData = df.filter(item => item.Station === station);
              traces.push({
                x: stationData.map(item => item.Tab_DateTime),
                y: stationData.map(item => item.Tab_Value_mDepthC1),
                type: 'scattergl',
                mode: 'lines',
                name: station,
                hoverinfo: 'x+y+name'
              });
            });
          } else {
            traces.push({
              x: df.map(item => item.Tab_DateTime),
              y: df.map(item => item.Tab_Value_mDepthC1),
              type: 'scattergl',
              mode: 'lines',
              name: `Sea Level Data (${filters.station})`,
              hoverinfo: 'x+y'
            });

            if (filters.showAnomalies) {
              const anomalies = df.filter(item => item.anomaly === -1);
              if (anomalies.length > 0) {
                traces.push({
                  x: anomalies.map(item => item.Tab_DateTime),
                  y: anomalies.map(item => item.Tab_Value_mDepthC1),
                  type: 'scattergl',
                  mode: 'markers',
                  name: 'Anomalies',
                  marker: { color: 'red', symbol: 'x', size: 8 }
                });
              }
            }

            if (filters.predictionModels.length > 0) {
              const predResponse = await fetch(`${apiBaseUrl}/predictions?station=${filters.station}&model=${filters.predictionModels.join(',')}`);
              const predData = await predResponse.json();

              if (predData.arima) {
                const lastDate = new Date(df[df.length - 1].Tab_DateTime);
                const futureDates = Array.from({ length: 240 }, (_, i) => {
                  const date = new Date(lastDate);
                  date.setHours(date.getHours() + i + 1);
                  return date;
                });
                traces.push({
                  x: futureDates,
                  y: predData.arima,
                  type: 'scattergl',
                  mode: 'lines',
                  name: 'ARIMA Forecast',
                  line: { dash: 'dot', color: 'lime' }
                });
              }

              if (predData.prophet) {
                traces.push({
                  x: predData.prophet.map(item => new Date(item.ds)),
                  y: predData.prophet.map(item => item.yhat),
                  type: 'scattergl',
                  mode: 'lines',
                  name: 'Prophet Forecast',
                  line: { dash: 'dot', color: 'orange' }
                });
              }
            }
          }

          // Calculate stats
          const stats = {
            current_level: df.length > 0 ? df[df.length - 1].Tab_Value_mDepthC1 : null,
            '24h_change': df.length > 1 ? df[df.length - 1].Tab_Value_mDepthC1 - df[0].Tab_Value_mDepthC1 : null,
            avg_temp: df.length > 0 ? df.reduce((sum, item) => sum + (item.Tab_Value_monT2m || 0), 0) / df.length : null,
            anomalies: df.filter(item => item.anomaly === -1).length
          };
          setStats(stats);
        } else {
          traces.push({
            x: data.map(item => new Date(item.Date)),
            y: data.map(item => item.HighTide),
            type: 'scatter',
            mode: 'lines',
            name: 'High Tide (m)',
            line: { color: 'deepskyblue' },
            hoverinfo: 'x+y+name'
          });
          traces.push({
            x: data.map(item => new Date(item.Date)),
            y: data.map(item => item.LowTide),
            type: 'scatter',
            mode: 'lines',
            name: 'Low Tide (m)',
            line: { color: 'lightcoral' },
            hoverinfo: 'x+y+name'
          });
          setStats({ current_level: null, '24h_change': null, avg_temp: null, anomalies: null });
        }

        setGraphData(traces);
      } catch (error) {
        console.error('Error fetching graph data:', error);
        setGraphData([]);
        setStats({ current_level: null, '24h_change': null, avg_temp: null, anomalies: null });
      }
    };

    fetchData();
  }, [filters, apiBaseUrl]);

  return (
    <Plot
      data={graphData}
      layout={{
        title: filters.dataType === 'default' ? 'Sea Level Over Time' : 'Tides Over Time',
        xaxis: { title: 'Date', linecolor: '#7FD1AE', gridcolor: '#1e3c72', gridwidth: 0.5 },
        yaxis: { title: filters.dataType === 'default' ? 'Sea Level (m)' : 'Tide Level (m)', linecolor: '#7FD1AE', gridcolor: '#1e3c72', gridwidth: 0.5 },
        showlegend: true,
        plot_bgcolor: '#142950',
        paper_bgcolor: '#0c1c35',
        font: { color: 'white' },
        hovermode: 'x unified',
        legend: { bgcolor: '#1e3c70', font: { size: 10 } }
      }}
      style={{ width: '100%', height: '100%' }}
      config={{ scrollZoom: true }}
    />
  );
};

export default GraphView;
```

### **`frontend/src/components/TableView.js`**

```jsx
import React, { useState, useEffect } from 'react';
import { Table } from 'react-bootstrap';
import moment from 'moment';

const TableView = ({ filters, apiBaseUrl }) => {
  const [tableData, setTableData] = useState([]);
  const [columns, setColumns] = useState([]);

  useEffect(() => {
    const fetchData = async () => {
      try {
        const response = await fetch(
          `${apiBaseUrl}/data?station=${filters.station}&start_date=${moment(filters.startDate).format('YYYY-MM-DD')}&end_date=${moment(filters.endDate).format('YYYY-MM-DD')}&data_source=${filters.dataType}`
        );
        const data = await response.json();
        if (!data || data.length === 0) {
          setTableData([]);
          setColumns([{ name: 'Status', id: 'Status' }]);
          return;
        }

        const columnMapping = filters.dataType === 'tides' ? {
          "Date": "Date",
          "Station": "Station",
          "HighTide": "High Tide (m)",
          "HighTideTime": "High Tide Time",
          "HighTideTemp": "High Tide Temp (°C)",
          "LowTide": "Low Tide (m)",
          "LowTideTime": "Low Tide Time",
          "LowTideTemp": "Low Tide Temp (°C)",
          "MeasurementCount": "Measurement Count"
        } : {
          "Tab_DateTime": "Date/Time",
          "Station": "Station",
          "Tab_Value_mDepthC1": "Sea Level (m)",
          "Tab_Value_monT2m": "Water Temp (°C)"
        };

        const cols = Object.keys(data[0]).map(col => ({
          name: columnMapping[col] || col,
          id: col
        }));
        setColumns(cols);
        setTableData(data);
      } catch (error) {
        console.error('Error fetching table data:', error);
        setTableData([]);
        setColumns([{ name: 'Status', id: 'Status' }]);
      }
    };

    fetchData();
  }, [filters, apiBaseUrl]);

  return (
    <Table striped bordered hover responsive className="dash-table">
      <thead>
        <tr>
          {columns.map(col => (
            <th key={col.id}>{col.name}</th>
          ))}
        </tr>
      </thead>
      <tbody>
        {tableData.length > 0 ? tableData.map((row, index) => (
          <tr key={index}>
            {columns.map(col => (
              <td key={col.id}>{typeof row[col.id] === 'number' ? row[col.id].toFixed(3) : row[col.id]}</td>
            ))}
          </tr>
        )) : (
          <tr>
            <td colSpan={columns.length}>No data available</td>
          </tr>
        )}
      </tbody>
    </Table>
  );
};

export default TableView;
```

### **`frontend/src/components/MapView.js`**

```jsx
import React, { useState, useEffect } from 'react';
import { MapContainer, TileLayer, Marker, Popup } from 'react-leaflet';
import 'leaflet/dist/leaflet.css';
import L from 'leaflet';
import moment from 'moment';

// Fix for default marker icons
delete L.Icon.Default.prototype._getIconUrl;
L.Icon.Default.mergeOptions({
  iconRetinaUrl: 'https://unpkg.com/leaflet@1.9.4/dist/images/marker-icon-2x.png',
  iconUrl: 'https://unpkg.com/leaflet@1.9.4/dist/images/marker-icon.png',
  shadowUrl: 'https://unpkg.com/leaflet@1.9.4/dist/images/marker-shadow.png',
});

const MapView = ({ filters, apiBaseUrl }) => {
  const [mapData, setMapData] = useState([]);

  useEffect(() => {
    const fetchData = async () => {
      try {
        const response = await fetch(
          `${apiBaseUrl}/data?station=${filters.station}&start_date=${moment(filters.startDate).format('YYYY-MM-DD')}&end_date=${moment(filters.endDate).format('YYYY-MM-DD')}&data_source=default`
        );
        const data = await response.json();

        const stationsResponse = await fetch(`${apiBaseUrl}/stations/map`);
        const stationsData = await stationsResponse.json();

        if (!data || !stationsData) {
          setMapData([]);
          return;
        }

        const latestValues = data.reduce((acc, item) => {
          if (!acc[item.Station] || new Date(item.Tab_DateTime) > new Date(acc[item.Station].Tab_DateTime)) {
            acc[item.Station] = item;
          }
          return acc;
        }, {});

        const mergedData = stationsData.map(station => ({
          ...station,
          latest_value: latestValues[station.Station]?.Tab_Value_mDepthC1 || 'N/A',
          last_update: latestValues[station.Station]?.Tab_DateTime ? moment(latestValues[station.Station].Tab_DateTime).format('YYYY-MM-DD HH:mm') : 'N/A'
        }));

        setMapData(mergedData);
      } catch (error) {
        console.error('Error fetching map data:', error);
        setMapData([]);
      }
    };

    fetchData();
  }, [filters, apiBaseUrl]);

  if (mapData.length === 0) {
    return <div>No station location data available</div>;
  }

  return (
    <MapContainer center={[31.5, 34.75]} zoom={6} style={{ height: '500px', width: '100%' }}>
      <TileLayer
        url="https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png"
        attribution='&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
      />
      {mapData.map(station => (
        <Marker key={station.Station} position={[station.latitude, station.longitude]}>
          <Popup>
            <b>{station.Station}</b><br />
            Sea Level (m): {typeof station.latest_value === 'number' ? station.latest_value.toFixed(3) : station.latest_value}<br />
            Last Update: {station.last_update}
          </Popup>
        </Marker>
      ))}
    </MapContainer>
  );
};

export default MapView;
```

---

## **תשתית AWS**

### **API Gateway**
- **שם:** `SeaLevelAPI`
- **Endpoints:**
  - `/stations` → `get_stations`
  - `/yesterday/{station}` → `get_yesterday_data`
  - `/live` ו-`/live/{station}` → `get_live_data`
  - `/data` → `get_data`
  - `/predictions` → `get_predictions`
  - `/stations/map` → `get_station_map`
- **CORS:** מופעל לכל ה-endpoints.
- **Authorizer:** Cognito User Pool לכל ה-endpoints לאימות משתמשים.

### **Cognito User Pool**
- **שם:** `SeaLevelUsers`
- **מאפיינים:** `email`, `sub`
- **Client App:** תמיכה ב-OAuth 2.0 עם scopes: `openid`, `email`, `profile`.

### **Aurora PostgreSQL**
- **הגדרה:** Cluster עם Read Replicas לשיפור ביצועים.
- **סנכרון:** Lambda function (`DMS`) לשליפת נתונים ממקור PostgreSQL מקומי כל דקה.

### **S3 Buckets**
- **Frontend:** אחסון ב-S3 עם CloudFront ל-CDN.
- **ייצוא:** Bucket נפרד לקבצי CSV/JSON.

### **CloudWatch**
- **לוגים:** מופעל לכל פונקציות ה-Lambda.
- **התראות:** Alarms לשגיאות או זמן תגובה איטי.

---

## **פריסה**

### **Backend**
1. ארוז את תיקיית `shared` כ-AWS Lambda Layer.
2. פרוס כל פונקציית Lambda עם AWS CLI:
   ```bash
   aws lambda create-function --function-name get_stations --runtime python3.9 --handler main.handler --zip-file fileb://get_stations.zip --role arn:aws:iam::YOUR_ACCOUNT:role/lambda-role --layers arn:aws:lambda:us-east-1:YOUR_ACCOUNT:layer:shared-layer:1
   ```
3. הגדר את API Gateway לנתב בקשות לפונקציות ה-Lambda.
4. הגדר Cognito Authorizer ב-API Gateway.

### **Frontend**
1. בצע `npm run build` בתיקיית `frontend/`.
2. העלה את תיקיית `build` ל-S3:
   ```bash
   aws s3 sync ./build/ s3://sea-level-frontend-bucket
   ```
3. הגדר CloudFront לשרת את ה-Bucket עם HTTPS.

### **משתני סביבה**
- **Backend:** הגדר `DB_URI` בכל פונקציית Lambda.
- **Frontend:** עדכן את `API_BASE_URL` ב-`App.js` עם כתובת ה-API Gateway.

---

## **בדיקות ומוניטורינג**

1. **בדיקות:** השתמש ב-Postman לבדיקת כל endpoint עם טוקן Cognito.
2. **מוניטורינג:** הגדר CloudWatch Logs ו-Alarms.
3. **ביצועים:** שקול להוסיף ElastiCache (Redis) לקריאות תכופות.

---

זהו הפרויקט המוגמר המלא, כולל כל הקוד, המבנה וההוראות לפריסה. הוא עונה על כל הדרישות שהוצגו ומספק מערכת מודרנית, גמישה וסקיילבילית לניטור רמות הים.